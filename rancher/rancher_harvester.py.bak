import yaml
import itertools
import re
import logging
import traceback
import json
from datetime import date,datetime,timedelta

from django.conf import settings
from django.core.exceptions import ObjectDoesNotExist
from django.db import models as django_models
from django.utils import timezone
from django.db import transaction

from data_storage import ResourceConsumeClient, AzureBlobStorage,exceptions,LockSession
from . import models
from data_storage.utils import get_property
from .utils import set_fields,set_field,set_fields_from_config
from . import modeldata

logger = logging.getLogger(__name__)

RANCHER_FILE_RE=re.compile("(^|/)(ingress-|cronjob-|deployment-|daemonset-|persistentvolumeclaim-|persistentvolume-|namespace-|statefulset-|configmap-).+\.(yaml|yml)$")

VOLUMN_RE=re.compile("(^|/)persistentvolume-.+\.(yaml|yml)$")
VOLUMN_CLAIM_RE=re.compile("(^|/)persistentvolumeclaim.+\.(yaml|yml)$")
DEPLOYMENT_RE=re.compile("(^|/)deployment-.+\.(yaml|yml)$")
CRONJOB_RE=re.compile("(^|/)cronjob-.+\.(yaml|yml)$")
DAEMONSET_RE=re.compile("(^|/)daemonset-.+\.(yaml|yml)$")
NAMESPACE_RE=re.compile("(^|/)namespace-.+\.(yaml|yml)$")
INGRESS_RE=re.compile("(^|/)ingress-.+\.(yaml|yml)$")
STATEFULSET_RE=re.compile("(^|/)statefulset-.+\.(yaml|yml)$")
CONFIGMAP_RE=re.compile("(^|/)configmap-.+\.(yaml|yml)$")

harvestername = "clusterconfig({})"

class JSONEncoder(json.JSONEncoder):
    """
    A JSON encoder to support encode datetime
    """
    def default(self,obj):
        from data_storage.settings import TZ
        if isinstance(obj,datetime):
            return obj.astimezone(tz=TZ).strftime("%Y-%m-%d %H:%M:%S.%f")
        elif isinstance(obj,date):
            return obj.strftime("%Y-%m-%d")
        elif isinstance(obj,django_models.Model):
            return str(obj)

        return json.JSONEncoder.default(self,obj)

_consume_clients = {}
def get_client(cluster,cache=True):
    """
    Return the blob resource client
    """
    if cluster not in _consume_clients or not cache:
        client = ResourceConsumeClient(
            AzureBlobStorage(settings.RANCHER_STORAGE_CONNECTION_STRING,settings.RANCHER_CONTAINER),
            settings.RANCHER_RESOURCE_NAME,
            settings.RESOURCE_CLIENTID,
            resource_base_path="{}/{}".format(settings.RANCHER_RESOURCE_NAME,cluster)

        )
        if cache:
            _consume_clients[cluster] = client
        else:
            return client
    return _consume_clients[cluster]

def update_project(cluster,projectid):
    if not projectid:
        return None
    try:
        obj = models.Project.objects.get(cluster=cluster,projectid=projectid)
    except ObjectDoesNotExist as ex:
        obj = models.Project(cluster=cluster,projectid=projectid)

    update_fields = None

    if obj.pk is None:
        obj.save()
        logger.debug("Create project({})".format(obj))
    elif update_fields:
        obj.save(update_fields=update_fields)
        logger.debug("Update project({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The project({}) is not changed".format(obj))

    return obj

def update_namespace(cluster,status,metadata,config):
    namespace_id = get_property(config,("metadata","annotations","field.cattle.io/projectId"))

    name = config["metadata"]["name"] or ""
    try:
        obj = models.Namespace.objects.get(cluster=cluster,name=name)
    except ObjectDoesNotExist as ex:
        obj = models.Namespace(cluster=cluster,name=name)

    update_fields = set_fields_from_config(obj,config,[
        ("deleted",None,lambda obj:None),
        ("added_by_log",None,lambda obj:False),
        ("api_version","apiVersion",None),
        ("project",("metadata","labels","field.cattle.io/projectId"),lambda val:update_project(cluster,val)),
        ("modified",("metadata","creationTimestamp"),lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) )
    ])
    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        logger.debug("Create namespace({})".format(obj))
    elif update_fields:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update namespace({}),update_fields={}".format(obj,update_fields))
        if "project" in update_fields :
            #namespace's project is changed, 
            #update models.PersistentVolumeClaim
            models.PersistentVolumeClaim.objects.filter(cluster=cluster,namespace=obj).update(project=obj.project)
            #update models.Ingress
            models.Ingress.objects.filter(cluster=cluster,namespace=obj).update(project=obj.project)
            #update models.Workload
            models.Workload.objects.filter(cluster=cluster,namespace=obj).update(project=obj.project)

    else:
        logger.debug("The namespace({}) is not changed".format(obj))


    #try to update the clusterid if it is empty or not match
    if namespace_id and ":" in namespace_id:
        cluster_id,project_id = namespace_id.split(':',1)
        if project_id == obj.project.projectid:
            if cluster.clusterid != cluster_id:
                cluster.clusterid = cluster_id
                cluster.save(update_fields=["clusterid"])

def delete_namespace(cluster,status,metadata,config):
    name = config["metadata"]["name"]
    
    obj = models.Namespace.objects.filter(cluster=cluster,name=name).first()
    if obj:
        obj.logically_delete()
        logger.info("Logically delete namespace({}.{})".format(cluster,name))

    """
    del_objs = models.Namespace.objects.filter(cluster=cluster,name=name).delete()
    if del_objs[0]:
        logger.info("Delete namespace({}),deleted objects = {}".format(name,del_objs))
    """

def update_configmap(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.get(cluster=cluster,name=namespace)

    name = config["metadata"]["name"]

    try:
        obj = models.ConfigMap.objects.get(cluster=cluster,namespace=namespace,name=name)
    except ObjectDoesNotExist as ex:
        obj = models.ConfigMap(cluster=cluster,namespace=namespace,name=name)

    update_fields = set_fields_from_config(obj,config,[
        ("api_version","apiVersion",None),
        ("modified",("metadata","creationTimestamp"),lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) )
    ])
    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        logger.debug("Create configmap({})".format(obj))
    elif update_fields:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update configmap({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The configmap({}) is not changed".format(obj))
    
    #save configmap items
    for key,value in config.get("data",{}).items():
        try:
            item = models.ConfigMapItem.objects.get(configmap=obj,name=key)
        except ObjectDoesNotExist as ex:
            item = models.ConfigMapItem(configmap=obj,name=key)

        item_update_fields = set_fields(item,[
            ("value",value)
        ])
        if item.pk is None:
            item.created = obj.modified
            item.modified = obj.modified
            item.save()
        elif item_update_fields:
            item_update_fields.append("refreshed")
            item_update_fields.append("modified")
            item.modified = obj.modified
            item.save(update_fields=item_update_fields)
        
def delete_configmap(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.filter(cluster=cluster,name=namespace).first()
    if not namespace:
        return
    
    name = config["metadata"]["name"]
    del_objs = models.ConfigMap.objects.filter(cluster=cluster,namespace=namespace,name=name).delete()
    if del_objs[0]:
        logger.info("Delete models.ConfigMap({}.{}),deleted objects = {}".format(namespace,name,del_objs))

def _get_ingress_protocol(val):
    if "http" in val:
        return "http"
    else:
        raise Exception("Failed to extract ingress protocol from {}".format(val))


def update_ingress_rules(ingress,configs):
    if not configs:
        del_objs = models.IngressRule.objects.filter(ingress=ingress).delete()
        if del_objs[0]:
            logger.debug("Delete the rules for models.Ingress({}),deleted objects = {}".format(ingress,del_objs))
        return

    name = None
    rule_ids = []
    for config in configs:
        hostname = config["host"]
        protocol = _get_ingress_protocol(config)
        for backend in get_property(config,(protocol,"paths")):
            path = backend.get("path","")
            try:
                obj = models.IngressRule.objects.get(ingress=ingress,protocol=protocol,hostname=hostname,path=path)
            except ObjectDoesNotExist as ex:
                obj = models.IngressRule(ingress=ingress,protocol=protocol,hostname=hostname,path=path,cluster=ingress.cluster)
            update_fields = set_fields_from_config(obj,backend,[
                ("servicename",("backend","serviceName"),lambda val: "{}:{}".format(ingress.namespace.name,val)),
                ("serviceport",("backend","servicePort"),lambda val:int(val))
            ])

            if obj.pk is None:
                obj.modified = ingress.modified
                obj.created = ingress.modified
                obj.save()
                rule_ids.append(obj.pk)
                logger.debug("Create deployment workload env({})".format(obj))
            elif update_fields:
                obj.modified = ingress.modified
                update_fields.append("modified")
                update_fields.append("refreshed")
                obj.save(update_fields=update_fields)
                rule_ids.append(obj.pk)
                logger.debug("Update the deployment workload env({}),update_fields={}".format(obj,update_fields))
            else:
                rule_ids.append(obj.pk)
                logger.debug("The deployment workload env({}) is not changed".format(obj))

    del_objs = models.IngressRule.objects.filter(ingress=ingress).exclude(pk__in=rule_ids).delete()
    if del_objs[0]:
        logger.debug("Delete the rules for models.Ingress({}),deleted objects = {}".format(ingress,del_objs))


def update_ingress(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.get(cluster=cluster,name=namespace)
    name = config["metadata"]["name"]
    try:
        obj = models.Ingress.objects.get(cluster=cluster,namespace=namespace,name=name)
    except ObjectDoesNotExist as ex:
        obj = models.Ingress(cluster=cluster,namespace=namespace,name=name)
    update_fields = set_fields_from_config(obj,config,[
        ("deleted",None,lambda obj:None),
        ("api_version","apiVersion",None),
        ("project",None,lambda val:namespace.project),
        ("modified",("metadata","creationTimestamp"),lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) ),
    ])

    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        logger.debug("Create models.Ingress({})".format(obj))
    elif update_fields:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update models.Ingress({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The models.Ingress({}) is not changed".format(obj))

    #update rules
    update_ingress_rules(obj,get_property(config,("spec","rules")))

def delete_ingress(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.filter(cluster=cluster,name=namespace).first()
    if not namespace:
        return
    name = config["metadata"]["name"]
    obj = models.Ingress.objects.filter(cluster=cluster,namespace=namespace,name=name).first() 
    if obj:
        obj.logically_delete()
        logger.info("Logically delete models.Ingress({}.{})".format(namespace,name))
    """
    del_objs = models.Ingress.objects.filter(cluster=cluster,namespace=namespace,name=name).delete()
    if del_objs[0]:
        logger.info("Delete models.Ingress({}),deleted objects = {}".format(name,del_objs))
    """

def _get_volume_uuid(val):
    if val.startswith("pvc-"):
        return val[4:]
    else:
        return val

def _get_volume_capacity(val):
    """
    Return the capacith with unit 'M'
    """
    if val.lower().endswith("gi"):
        return int(val[:-2]) * 1024
    elif val.lower().endswith("mi"):
        return int(val[:-2]) * 1024
    else:
        raise Exception("Parse storage capacity({}) failed".format(val))

def _get_volume_storage_class_name(val):
    storage_class = get_property(val,("spec","storageClassName"))
    if not storage_class:
        if get_property(val,("spec","nfs")):
            storage_class = "nfs-client"
    
        if get_property(val,("spec","volumeMode")) == "Filesystem":
            storage_class = "local-path"

    return storage_class


def _get_volume_path(val):
    storage_class = _get_volume_storage_class_name(val)
    if storage_class == "local-path":
        return get_property(val,("spec","hostPath","path"))
    elif storage_class == "nfs-client":
        return "{}:{}".format(get_property(val,("spec","nfs","server")),get_property(val,("spec","nfs","path")))
    elif storage_class == "default":
        return get_property(val,("spec","azureDisk","diskURI"))
    else:
        return "Storage class({}) Not support".format(storage_class)

def update_volume(cluster,status,metadata,config):
    name = config["metadata"]["name"]
    try:
        obj = models.PersistentVolume.objects.get(cluster=cluster,name=name)
    except ObjectDoesNotExist as ex:
        obj = models.PersistentVolume(cluster=cluster,name=name)
    update_fields = set_fields_from_config(obj,config,[
        ("deleted",None,lambda obj:None),
        ("api_version","apiVersion",None),
        ("kind","kind",None),
        ("modified",("metadata","creationTimestamp"),lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) ),
        ("writable",("spec","accessModes"),lambda val:True if next((v for v in val if "write" in v.lower()),None) else False),
        ("storage_class_name",None,_get_volume_storage_class_name),
        ("volume_mode",("spec","volumeMode"),None),
        ("uuid",("metadata","name"),_get_volume_uuid),
        ("volumepath",None,_get_volume_path),
        ("capacity",("spec","capacity","storage"),_get_volume_capacity),
        ("reclaim_policy",("spec","persistentVolumeReclaimPolicy"),None),
        ("node_affinity",("spec","nodeAffinity"),lambda val:yaml.dump(val)),
    ])

    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        logger.debug("Create models.PersistentVolume({})".format(obj))
    elif update_fields:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update models.PersistentVolume({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The models.PersistentVolume({}) is not changed".format(obj))


def delete_volume(cluster,status,metadata,config):
    name = config["metadata"]["name"]
    obj = models.PersistentVolume.objects.filter(cluster=cluster,name=name).first()
    if obj:
        obj.logically_delete()
        logger.info("Logically Delete models.PersistentVolume({}.{})".format(cluster,name))
    """
    del_objs = models.PersistentVolume.objects.filter(cluster=cluster,name=name).delete()
    if del_objs[0]:
        logger.info("Delete models.PersistentVolume({}),deleted objects = {}".format(name,del_objs))
    """

def update_volume_claim(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.get(cluster=cluster,name=namespace)
    name = config["metadata"]["name"]
    try:
        obj = models.PersistentVolumeClaim.objects.get(cluster=cluster,namespace=namespace,name=name)
    except ObjectDoesNotExist as ex:
        obj = models.PersistentVolumeClaim(cluster=cluster,namespace=namespace,name=name)
    update_fields = set_fields_from_config(obj,config,[
        ("deleted",None,lambda obj:None),
        ("api_version","apiVersion",None),
        ("project",None,lambda val:namespace.project),
        ("modified",("metadata","creationTimestamp"),lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) ),
        ("writable",("spec","accessModes"),lambda val:True if next((v for v in val if "write" in v.lower()),None) else False),
        ("volume",("spec","volumeName"),lambda val: models.PersistentVolume.objects.get(cluster=cluster,name=val) if val else None),
    ])

    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        logger.debug("Create models.PersistentVolumeClaim({})".format(obj))
    elif update_fields:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update models.PersistentVolumeClaim({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The models.PersistentVolumeClaim({}) is not changed".format(obj))

def delete_volume_claim(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.filter(cluster=cluster,name=namespace).first()
    if not namespace:
        return
    name = config["metadata"]["name"]
    obj = models.PersistentVolumeClaim.objects.filter(cluster=cluster,namespace=namespace,name=name).first()
    if obj:
        obj.logically_delete()
        logger.info("Logically delete models.PersistentVolumeClaim({}.{})".format(namespace,name))
    """
    del_objs = models.PersistentVolumeClaim.objects.filter(cluster=cluster,namespace=namespace,name=name).delete()
    if del_objs[0]:
        logger.info("Delete models.PersistentVolumeClaim({}),deleted objects = {}".format(name,del_objs))
    """

def update_workload_envs(workload,config,env_configs,envfrom_configs=None):
    """
    Return True if some env is updated;otherwise return False
    """
    if not env_configs and not envfrom_configs:
        del_objs = models.WorkloadEnv.objects.filter(workload=workload).delete()
        if del_objs[0]:
            logger.debug("Delete the envs for workload({}),deleted objects = {}".format(workload,del_objs))
            return True
        else:
            return False

    def _get_env_value(env_config):
        if "value" in env_config:
            return (env_config["value"],None,None)
        elif "valueFrom" in env_config:
            if "fieldRef" in env_config["valueFrom"]:
                val = get_property(config,tuple(env_config["valueFrom"]["fieldRef"]["fieldPath"].split(".")))
                if val is None:
                    return (yaml.dump(env_config["valueFrom"]),None,None)
                else:
                    return (val,None,None)
            elif "configMapKeyRef" in env_config["valueFrom"]:
                #env from configmap
                configmap_config = env_config["valueFrom"]["configMapKeyRef"]
                configmap_name = configmap_config["name"]
                configmap = models.ConfigMap.objects.get(cluster=workload.cluster,namespace=workload.namespace,name=configmap_name)
                configitem = models.ConfigMapItem.objects.filter(configmap=configmap,name=configmap_config["key"]).first()
                return (configitem.value if configitem else None,configmap,configitem)
        elif len(env_config) == 1:
            return (None,None,None)
        else:
            return (yaml.dump(env_config),None,None)

    updated = False
    name = None
    env_ids = []
    env_names = []
    #update env from env_configs
    for env_config in env_configs or []:
        name = env_config["name"]
        env_names.append(name)
        try:
            obj = models.WorkloadEnv.objects.get(workload=workload,name=name)
        except ObjectDoesNotExist as ex:
            obj = models.WorkloadEnv(workload=workload,name=name)

        value,configmap,configmapitem = _get_env_value(env_config)

        update_fields = set_fields(obj,[
            ("value",value),
            ("configmap",configmap),
            ("configmapitem",configmapitem)
        ])

        if obj.pk is None:
            obj.modified = workload.modified
            obj.created = workload.modified
            obj.save()
            updated = True
            logger.debug("Create deployment workload env({})".format(obj))
        elif update_fields:
            obj.modified = workload.modified
            update_fields.append("modified")
            update_fields.append("refreshed")
            obj.save(update_fields=update_fields)
            updated = True
            logger.debug("Update the deployment workload env({}),update_fields={}".format(obj,update_fields))
        else:
            logger.debug("The deployment workload env({}) is not changed".format(obj))
        env_ids.append(obj.id)

    #update env from envfrom_configs
    for envfrom_config in envfrom_configs or []:
        if "configMapRef" in envfrom_config:
            #env from configMap
            prefix = envfrom_config.get("prefix") or None
            configmap_name = envfrom_config["configMapRef"]["name"]
            configmap = models.ConfigMap.objects.get(cluster=workload.cluster,namespace=workload.namespace,name=configmap_name)
            configitem_qs = models.ConfigMapItem.objects.filter(configmap=configmap)
            if prefix:
                configitem_qs = configitem_qs.filter(name__startswith=prefix)
            for configitem in configitem_qs:
                if configitem.name in env_names:
                    #already declared in workload,ignore
                    continue
                try:
                    obj = models.WorkloadEnv.objects.get(workload=workload,name=configitem.name)
                except ObjectDoesNotExist as ex:
                    obj = models.WorkloadEnv(workload=workload,name=configitem.name)

                update_fields = set_fields(obj,[
                    ("value",configitem.value),
                    ("configmap",configmap),
                    ("configmapitem",configitem)
                ])

                if obj.pk is None:
                    obj.modified = workload.modified
                    obj.created = workload.modified
                    obj.save()
                    updated = True
                    logger.debug("Create deployment workload env({})".format(obj))
                elif update_fields:
                    obj.modified = workload.modified
                    update_fields.append("modified")
                    update_fields.append("refreshed")
                    obj.save(update_fields=update_fields)
                    updated = True
                    logger.debug("Update the deployment workload env({}),update_fields={}".format(obj,update_fields))
                else:
                    logger.debug("The deployment workload env({}) is not changed".format(obj))
                env_ids.append(obj.id)

    del_objs = models.WorkloadEnv.objects.filter(workload=workload).exclude(id__in=env_ids).delete()
    if del_objs[0]:
        logger.debug("Delete the envs for workload({}),deleted objects = {}".format(workload,del_objs))
        updated = True
    return updated

def update_workload_listenings(workload,config):
    """
    Return True if some env is updated;otherwise return False
    """
    #get listen config from public endpoints
    listen_configs = get_property(config,("metadata","annotations","field.cattle.io/publicEndpoints"),lambda val: json.loads(val) if val else None)
    if listen_configs:
        #if serviceName is not provided(daemonset has not serviceName in its listen config), populate one.
        for listen_config in listen_configs:
            if "serviceName" not in listen_config:
                listen_config["serviceName"] = "{0}:{2}({1})".format(workload.namespace.name,listen_config["protocol"].lower(),listen_config["port"])
    else:
        #if can't get the listen config from public endpoints, try to find them in specification
        listen_configs = []
        for port_config in get_property(config,("spec","template","spec",'containers',0,"ports")) or []:
            listen_configs.append({
                "serviceName":"{0}:{2}({1})".format(workload.namespace.name,port_config.get("protocol","").lower(),port_config.get("containerPort",0)),
                "port":int(port_config.get("containerPort",0)),
                "protocol":port_config.get("protocol","").lower()
            })
        if not listen_configs:
            del_objs = models.WorkloadListening.objects.filter(workload=workload).delete()
            if del_objs[0]:
                logger.debug("Delete the listenings for workload({}),deleted objects = {}".format(workload,del_objs))
                return True
            else:
                return False

    updated = False
    name = None

    listen_ids = []
    for listen_config in listen_configs:
        servicename = listen_config["serviceName"]
        if "ingressName" in listen_config:
            #ingress router
            ingress_namespace,ingressname = listen_config["ingressName"].split(":")
            ingress_namespace = models.Namespace.objects.get(cluster=workload.cluster,name=ingress_namespace)
            ingress = models.Ingress.objects.get(cluster=workload.cluster,namespace=ingress_namespace,name=ingressname)
            ingress_rule = models.IngressRule.objects.get(ingress=ingress,servicename=listen_config["serviceName"])
            try:
                obj = models.WorkloadListening.objects.get(workload=workload,servicename=servicename,ingress_rule=ingress_rule)
            except ObjectDoesNotExist as ex:
                obj = models.WorkloadListening(workload=workload,servicename=servicename,ingress_rule=ingress_rule)
        else:
            ingress_rule = None
            try:
                obj = models.WorkloadListening.objects.get(workload=workload,servicename=servicename,ingress_rule__isnull=True)
            except ObjectDoesNotExist as ex:
                obj = models.WorkloadListening(workload=workload,servicename=servicename,ingress_rule=None)

        update_fields = set_fields_from_config(obj,listen_config,[
            ("servicename","serviceName",None),
            ("listen_port","port",lambda val:int(val)),
            ("protocol","protocol",lambda val: val.lower() if val else None),
        ])

        #try to find the ingressName or container port
        if ingress_rule:
            #ingress router
            set_field(obj,"container_port", ingress_rule.serviceport,update_fields)
            #try to update the ingressrule's port if not match
            if ingress_rule.port != obj.listen_port:
                ingress_rule.port = obj.listen_port
                ingress_rule.save(update_fields=["port"])
            if ingress_rule.cluster.ip:
                if listen_config["addresses"] and ingress_rule.cluster.ip not in listen_config["addresses"]:
                    ingress_rule.cluster.ip = listen_config["addresses"][0]
                    ingress_rule.cluster.save(update_fields=["ip"])
            elif listen_config["addresses"]:
                ingress_rule.cluster.ip = listen_config["addresses"][0]
                ingress_rule.cluster.save(update_fields=["ip"])
        else:
            #port mapping
            container_port = None
            dns_name = obj.servicename.split(':',1)[1]
            for ports_config in json.loads(get_property(config,("spec","template","metadata","annotations","field.cattle.io/ports")) or "[]"):
                for port_config in ports_config:
                    if port_config["dnsName"] == dns_name:
                        container_port = int(port_config["containerPort"])
                        break
                if container_port:
                    break
            if not container_port:
                for port_config in get_property(config,("spec","template","spec",'containers',0,"ports")) or []:
                    if int(port_config.get("containerPort",0)) == obj.listen_port and port_config.get("protocol","").lower() == obj.protocol:
                        container_port = obj.listen_port
                        break

            if not container_port:
                raise Exception("Failed to find the container port for the public port({})".format(obj.listen_port))
            set_field(obj,"container_port", container_port,update_fields)

        if obj.pk is None:
            obj.modified = workload.modified
            obj.created = workload.modified
            obj.save()
            updated = True
            logger.debug("Create workload listening({})".format(obj))
        elif update_fields:
            obj.modified = workload.modified
            update_fields.append("modified")
            update_fields.append("refreshed")
            obj.save(update_fields=update_fields)
            updated = True
            logger.debug("Update the workload listening({}),update_fields={}".format(obj,update_fields))
        else:
            logger.debug("The workload listening({}) is not changed".format(obj))
        listen_ids.append(obj.id)

    # remove the not deleted listenings from db
    del_objs = models.WorkloadListening.objects.filter(workload=workload).exclude(id__in=listen_ids).delete()
    if del_objs[0]:
        logger.debug("Delete the listenings for workload({}),deleted objects = {}".format(workload,del_objs))
        updated = True

    return updated


def update_workload_volumes(workload,config,spec_config):
    """
    Return True if some env is updated;otherwise return False
    """
    volumemount_configs = get_property(spec_config,("containers",0,"volumeMounts"))
    if not volumemount_configs:
        del_objs = models.WorkloadVolume.objects.filter(workload=workload).delete()
        if del_objs[0]:
            logger.debug("Delete the volumes for workload({}),deleted objects = {}".format(workload,del_objs))
            return True
        else:
            return False

    updated = False
    name = None
    del_objs = models.WorkloadVolume.objects.filter(workload=workload).exclude(name__in=[c["name"] for c in volumemount_configs]).delete()
    if del_objs[0]:
        logger.debug("Delete the volumes for workload({}),deleted objects = {}".format(workload,del_objs))
        updated = True

    #exact all volumes from yaml file
    volume_configs = {}
    for volume_config in get_property(spec_config,"volumes") or []:
        volume_configs[volume_config["name"]] = volume_config

    for volumemount_config in volumemount_configs:
        name = volumemount_config["name"]
        try:
            obj = models.WorkloadVolume.objects.get(workload=workload,name=name)
        except ObjectDoesNotExist as ex:
            obj = models.WorkloadVolume(workload=workload,name=name)

        writable = get_property(volumemount_config,"readOnly",lambda val: False if val else True)
        update_fields = set_fields_from_config(obj,volumemount_config,[
            ("mountpath","mountPath",None),
            ("subpath","subPath",None)
        ])
        if name not in volume_configs:
            continue
        volume_config = volume_configs[name]
        if "persistentVolumeClaim" in volume_config:
            #reference the volume from volume claim
            claimname = volume_config["persistentVolumeClaim"]["claimName"]
            set_field(obj,"volume_claim", models.PersistentVolumeClaim.objects.get(cluster=workload.cluster,namespace=workload.namespace,name=claimname),update_fields)
            set_field(obj,"volume", obj.volume_claim.volume,update_fields)
            set_field(obj,"volumepath", obj.volume_claim.volume.volumepath if obj.volume_claim.volume else None ,update_fields)
            set_field(obj,"other_config", None,update_fields)
            if writable:
                writable = obj.volume_claim.writable
        elif "hostPath" in volume_config:
            hostpath = volume_config["hostPath"]["path"]
            set_field(obj,"volume_claim", None,update_fields)
            set_field(obj,"volumepath", hostpath,update_fields)
            set_field(obj,"volume", models.PersistentVolume.objects.filter(cluster=workload.cluster,volumepath=hostpath).first(),update_fields)
            set_field(obj,"other_config", None,update_fields)
            if writable and obj.volume:
                writable = obj.volume.writable
        else:
            set_field(obj,"other_config", yaml.dump(volume_config),update_fields)

        set_field(obj,"writable",writable,update_fields)

        if obj.pk is None:
            obj.modified = workload.modified
            obj.created = workload.modified
            obj.save()
            updated = True
            logger.debug("Create deployment workload volume({})".format(obj))
        elif update_fields:
            obj.modified = workload.modified
            update_fields.append("modified")
            update_fields.append("refreshed")
            obj.save(update_fields=update_fields)
            updated = True
            logger.debug("Update the deployment workload volume({}),update_fields={}".format(obj,update_fields))
        else:
            logger.debug("The deployment workload volume({}) is not changed".format(obj))
    return updated

def update_deployment(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.get(cluster=cluster,name=namespace)
    name = config["metadata"]["name"]
    kind = get_property(config,"kind")
    imageid = get_property(config,("spec","template","spec","containers",0,"image"))
    image = models.ContainerImage.parse_imageid(imageid,scan=True)

    try:
        obj = models.Workload.objects.get(cluster=cluster,namespace=namespace,name=name,kind=kind)
    except ObjectDoesNotExist as ex:
        obj = models.Workload(cluster=cluster,namespace=namespace,name=name,kind=kind)
    update_fields = set_fields_from_config(obj,config,[
        ("deleted",None,lambda obj:None),
        ("added_by_log",None,lambda obj:False),
        ("api_version","apiVersion",None),
        ("project",None,lambda val:namespace.project),
        ("modified",[("spec","template","metadata","annotations","cattle.io/timestamp"),("metadata","creationTimestamp")],lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) ),
        ("replicas",("spec","replicas"),lambda val:int(val) if val else 0),
        ("containerimage",None,lambda obj:image),
        ("image",("spec","template","spec","containers",0,"image"),None),
        ("image_pullpolicy",("spec","template","spec","containers",0,"imagePullPolicy"),None),
        ("cmd",("spec","template","spec","containers",0,"args"), lambda val:json.dumps(val) if val else None),
        ("schedule",None,lambda val: None),
        ("failedjobshistorylimit", None,lambda val:None),
        ("successfuljobshistorylimit", None,lambda val:None),
        ("suspend", None,lambda val:None),
        ("concurrency_policy", None,lambda val:None)
    ])
    created = False
    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        created = True
        logger.debug("Create deployment workload({})".format(obj))

    #update envs
    updated = update_workload_envs(obj,config,get_property(config,("spec","template","spec","containers",0,"env")),get_property(config,("spec","template","spec","containers",0,"envFrom")))

    #update listenings
    updated = update_workload_listenings(obj,config) or updated

    #update volumes
    updated = update_workload_volumes(obj,config,get_property(config,("spec","template","spec"))) or updated

    if created:
        pass
    elif update_fields or updated:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update the deployment workload({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The deployment workload({}) is not changed".format(obj))

def delete_deployment(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.filter(cluster=cluster,name=namespace).first()
    if not namespace:
        return
    name = config["metadata"]["name"]
    kind = config["kind"]
    obj = models.Workload.objects.filter(cluster=cluster,namespace=namespace,name=name,kind=kind).first()
    if obj:
        obj.logically_delete()
        logger.info("Logically delete the deployment workload({2}:{0}.{1})".format(namespace,name,kind))
    """
    del_objs = models.Workload.objects.filter(cluster=cluster,namespace=namespace,name=name,kind=kind).delete()
    if del_objs[0]:
        logger.info("Delete the deployment workload({}),deleted objects = {}".format(name,del_objs))
    """

def update_cronjob(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.get(cluster=cluster,name=namespace)
    name = config["metadata"]["name"]
    kind = get_property(config,"kind")
    imageid = get_property(config,("spec","jobTemplate","spec","template","spec","containers",0,"image"))
    image = models.ContainerImage.parse_imageid(imageid,scan=True)
    try:
        obj = models.Workload.objects.get(cluster=cluster,namespace=namespace,name=name,kind=kind)
    except ObjectDoesNotExist as ex:
        obj = models.Workload(cluster=cluster,namespace=namespace,name=name,kind=kind)

    update_fields = set_fields_from_config(obj,config,[
        ("deleted",None,lambda obj:None),
        ("added_by_log",None,lambda obj:False),
        ("api_version","apiVersion",None),
        ("project",None,lambda val:namespace.project),
        ("modified",[("spec","jobTemplate","spec","template","metadata","annotations","cattle.io/timestamp"),("metadata","creationTimestamp")],lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) ),
        ("containerimage",None,lambda obj:image),
        ("image",("spec","jobTemplate","spec","template","spec","containers",0,"image"),None),
        ("image_pullpolicy",("spec","jobTemplate","spec","template","spec","containers",0,"imagePullPolicy"),None),
        ("replicas",None,lambda val:0),
        ("cmd",("spec","jobTemplate","spec","template","spec","containers",0,"args"), lambda val:json.dumps(val) if val else None),
        ("schedule",("spec","schedule"), None),
        ("failedjobshistorylimit", ("spec","failedJobsHistoryLimit"),lambda val:int(val) if val else 0),
        ("successfuljobshistorylimit", ("spec","failedJobsHistoryLimit"),lambda val:int(val) if val else 0),
        ("suspend", ("spec","suspend"),lambda val:True if val else False),
        ("concurrency_policy", ("spec","concurrencyPolicy"),None)
    ])

    created = False
    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        created = True
        logger.debug("Create cronjob workload({})".format(obj))

    #update envs
    updated = update_workload_envs(obj,config,get_property(config,("spec","jobTemplate","spec","template","spec","containers",0,"env")),get_property(config,("spec","jobTemplate","spec","template","spec","containers",0,"envFrom")))

    #delete all listening objects
    updated = update_workload_listenings(obj,config) or updated

    #update volumes
    updated = update_workload_volumes(obj,config,get_property(config,("spec","jobTemplate","spec","template","spec"))) or updated

    if created:
        pass
    elif update_fields or updated:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update the cronjob workload({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The cronjob workload({}) is not changed".format(obj))

def delete_cronjob(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.filter(cluster=cluster,name=namespace).first()
    if not namespace:
        return
    name = config["metadata"]["name"]
    kind = config["kind"]
    obj = models.Workload.objects.filter(cluster=cluster,namespace=namespace,name=name,kind=kind).first()
    if obj:
        obj.logically_delete()
        logger.info("Logically delete the cronjob workload({2}:{0}.{1})".format(namespace,name,kind))
    """
    del_objs = models.Workload.objects.filter(cluster=cluster,namespace=namespace,name=name,kind=kind).delete()
    if del_objs[0]:
        logger.info("Delete the cronjob workload({}),deleted objects = {}".format(name,del_objs))
    """

def update_daemonset(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.get(cluster=cluster,name=namespace)
    name = config["metadata"]["name"]
    kind = get_property(config,"kind")
    imageid = get_property(config,("spec","template","spec","containers",0,"image"))
    image = models.ContainerImage.parse_imageid(imageid,scan=True)

    try:
        obj = models.Workload.objects.get(cluster=cluster,namespace=namespace,name=name,kind=kind)
    except ObjectDoesNotExist as ex:
        obj = models.Workload(cluster=cluster,namespace=namespace,name=name,kind=kind)

    update_fields = set_fields_from_config(obj,config,[
        ("deleted",None,lambda obj:None),
        ("added_by_log",None,lambda obj:False),
        ("api_version","apiVersion",None),
        ("kind","kind",None),
        ("project",None,lambda val:namespace.project),
        ("modified",[("spec","template","metadata","annotations","cattle.io/timestamp"),("metadata","creationTimestamp")],lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) ),
        ("containerimage",None,lambda obj:image),
        ("image",("spec","template","spec","containers",0,"image"),None),
        ("image_pullpolicy",("spec","template","spec","containers",0,"imagePullPolicy"),None),
        ("replicas",None,lambda val:0),
        ("cmd",("spec","template","spec","containers",0,"args"), lambda val:json.dumps(val) if val else None),
        ("schedule",None, lambda val: None),
        ("failedjobshistorylimit", None,lambda val:None),
        ("successfuljobshistorylimit", None,lambda val:None),
        ("suspend", None,lambda val:None),
        ("concurrency_policy", None,lambda val:None)
    ])

    created = False
    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        created = True
        logger.debug("Create daemonset workload({})".format(obj))

    #update envs
    updated = update_workload_envs(obj,config,get_property(config,("spec","template","spec","containers",0,"env")),get_property(config,("spec","template","spec","containers",0,"envFrom")))

    #delete all listening objects
    updated = update_workload_listenings(obj,config) or updated

    #update volumes
    updated = update_workload_volumes(obj,config,get_property(config,("spec","template","spec"))) or updated

    if created:
        pass
    elif update_fields or updated:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update the daemon workload({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The daemon workload({}) is not changed".format(obj))

def delete_daemonset(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.filter(cluster=cluster,name=namespace).first()
    if not namespace:
        return
    name = config["metadata"]["name"]
    kind = config["kind"]
    obj = models.Workload.objects.filter(cluster=cluster,namespace=namespace,name=name,kind=kind).first()
    if obj:
        obj.logically_delete()
        logger.info("Logically delete the daemonset workload({2}:{0}.{1})".format(namespace,name,kind))
    """
    del_objs = models.Workload.objects.filter(cluster=cluster,namespace=namespace,name=name,kind=kind).delete()
    if del_objs[0]:
        logger.info("Delete the daemonset workload({}),deleted objects = {}".format(name,del_objs))
    """

def update_statefulset(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.get(cluster=cluster,name=namespace)
    name = config["metadata"]["name"]
    kind = get_property(config,"kind")
    imageid = get_property(config,("spec","template","spec","containers",0,"image"))
    image = models.ContainerImage.parse_imageid(imageid,scan=True)

    try:
        obj = models.Workload.objects.get(cluster=cluster,namespace=namespace,name=name,kind=kind)
    except ObjectDoesNotExist as ex:
        obj = models.Workload(cluster=cluster,namespace=namespace,name=name,kind=kind)

    update_fields = set_fields_from_config(obj,config,[
        ("deleted",None,lambda obj:None),
        ("added_by_log",None,lambda obj:False),
        ("api_version","apiVersion",None),
        ("project",None,lambda val:namespace.project),
        ("kind","kind",None),
        ("modified",[("spec","template","metadata","annotations","cattle.io/timestamp"),("metadata","creationTimestamp")],lambda dtstr:timezone.localtime(datetime.strptime(dtstr,"%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=timezone.timezone(offset=timedelta(hours=0)))) ),
        ("containerimage",None,lambda obj:image),
        ("image",("spec","template","spec","containers",0,"image"),None),
        ("image_pullpolicy",("spec","template","spec","containers",0,"imagePullPolicy"),None),
        ("replicas",None,lambda val:0),
        ("cmd",("spec","template","spec","containers",0,"args"), lambda val:json.dumps(val) if val else None),
        ("schedule",None, lambda val: None),
        ("failedjobshistorylimit", None,lambda val:None),
        ("successfuljobshistorylimit", None,lambda val:None),
        ("suspend", None,lambda val:None),
        ("concurrency_policy", None,lambda val:None)
    ])

    created = False
    if obj.pk is None:
        obj.created = obj.modified
        obj.save()
        created = True
        logger.debug("Create statefulset workload({})".format(obj))

    #update envs
    updated = update_workload_envs(obj,config,get_property(config,("spec","template","spec","containers",0,"env")),get_property(config,("spec","template","spec","containers",0,"envFrom")))

    #update listenings
    updated = update_workload_listenings(obj,config) or updated

    #update volumes
    updated = update_workload_volumes(obj,config,get_property(config,("spec","template","spec"))) or updated

    #update database server if it is a database server
    image_name_lower = obj.image.lower()
    for key,dbtype,default_port in (
        ("postgis",models.DatabaseServer.POSTGRES,5432),
        ("postgres",models.DatabaseServer.POSTGRES,5432),
        ("mysql",models.DatabaseServer.MYSQL,3306),
        ("oracle",models.DatabaseServer.ORACLE,1521)
    ):
        if key not in image_name_lower:
            continue

        #postgres related image
        listening =  obj.listenings.first()
        if listening:
            listen_port = listening.listen_port
            internal_port = listening.container_port
        elif "admin" in obj.name:
            #is a database admin
            continue
        else:
            listen_port = None
            internal_port = default_port
        #it is a postgres server
        database_server = update_databaseserver(obj.cluster.name,dbtype,listen_port,obj.modified,
            ip=obj.cluster.ip,
            internal_name="{}/{}".format(obj.namespace.name,obj.name),
            internal_port=internal_port,
            workload=obj
        )
        break
    if created:
        pass
    elif update_fields or updated:
        update_fields.append("refreshed")
        obj.save(update_fields=update_fields)
        logger.debug("Update the statefulset workload({}),update_fields={}".format(obj,update_fields))
    else:
        logger.debug("The statefulset workload({}) is not changed".format(obj))

def delete_statefulset(cluster,status,metadata,config):
    namespace = config["metadata"]["namespace"]
    namespace = models.Namespace.objects.filter(cluster=cluster,name=namespace).first()
    if not namespace:
        return
    name = config["metadata"]["name"]
    kind = config["kind"]
    obj = models.Workload.objects.filter(cluster=cluster,namespace=namespace,name=name,kind=kind).first()
    if obj:
        obj.logically_delete()
        logger.info("Logically delete the statefulset workload({2}:{0}.{1})".format(namespace,name,kind))
    """
    del_objs = models.Workload.objects.filter(cluster=cluster,namespace=namespace,name=name,kind=kind).delete()
    if del_objs[0]:
        logger.info("Delete the statefulset workload({}),deleted objects = {}".format(name,del_objs))
    """

def update_databaseserver(hostname,kind,port,modified,host=None,ip=None,internal_name=None,internal_port=None,workload=None):
    if not port:
        port = None
    else:
        port = int(port)
    try:
        if port:
            server = models.DatabaseServer.objects.get(host=hostname,port=port)
        elif internal_name:
            server = models.DatabaseServer.objects.get(host=hostname,internal_name=internal_name)
        else:
            server = models.DatabaseServer.objects.get(host=hostname)
    except ObjectDoesNotExist as ex:
        server = models.DatabaseServer(host=hostname,port=port,modified=modified)
    update_fields = []
    set_field(server,"ip", ip,update_fields)
    set_field(server,"kind", kind,update_fields)
    set_field(server,"workload", workload,update_fields)
    if internal_name:
        set_field(server,"internal_name",internal_name,update_fields)
    if internal_port:
        set_field(server,"internal_port",internal_port,update_fields)
    if host and host != hostname and (not server.other_names or host not in server.other_names):
        if server.other_names:
            server.other_names.append(host)
        else:
            server.other_names = [host]
        update_fields.append("other_names")

    if server.pk is None:
        server.created = server.modified
        server.save()
        logger.debug("Create database server({})".format(server))
    elif update_fields:
        update_fields.append("refreshed")
        server.save(update_fields=update_fields)
        logger.debug("Update database server({}),update_fields={}".format(server,update_fields))
    else:
        logger.debug("The database server({}) is not changed".format(server))

    return server


def update_database(server,name,modified):
    try:
        database = models.Database.objects.get(server=server,name=name)
    except ObjectDoesNotExist as ex:
        database = models.Database(server=server,name=name,created=modified)
        database.save()

    return database

def update_databaseuser(server,user,password,modified):
    try:
        database_user = models.DatabaseUser.objects.get(server=server,user=user)
    except ObjectDoesNotExist as ex:
        database_user = models.DatabaseUser(server=server,user=user,modified=modified)
    update_fields = []
    set_field(database_user,"password", password,update_fields)
    if database_user.pk is None:
        database_user.created = database_user.modified
        database_user.save()
        logger.debug("Create database user({})".format(database_user))
    elif update_fields:
        update_fields.append("refreshed")
        database_user.save(update_fields=update_fields)
        logger.debug("Update database user({}),update_fields={}".format(database_user,update_fields))
    else:
        logger.debug("The database user({}) is not changed".format(database_user))

    return database_user

def update_workloaddatabase(workload,database,user,password,config_items,modified,schema=None):
    """
    Return the workloaddatabase object
    """
    try:
        workload_database = models.WorkloadDatabase.objects.get(workload=workload,database=database,config_items=config_items)
    except ObjectDoesNotExist as ex:
        workload_database = models.WorkloadDatabase(workload=workload,database=database,modified=modified,config_items=config_items)
    update_fields = []
    set_field(workload_database,"user", user,update_fields)
    set_field(workload_database,"password", password,update_fields)
    set_field(workload_database,"schema", schema,update_fields)
    if workload_database.pk is None:
        workload_database.created = workload_database.modified
        workload_database.save()
        logger.debug("Create database({1}) for workload({0})".format(workload,database))
    elif update_fields:
        update_fields.append("refreshed")
        workload_database.save(update_fields=update_fields)
        logger.debug("Update database({1}) for workload({0}),update_fields={2}".format(workload,database,update_fields))
    else:
        logger.debug("The database({1}) for workload({0}) is not changed".format(workload,database))

    return workload_database


ip_re = re.compile("^[0-9]{1,3}(\.[0-9]{1,3}){3,3}$")
postgres_connection_string_re = re.compile('^\s*(?P<database>(postgis)|(postgres))://(?P<user>[a-zA-Z0-9@\-_\.]+)(:(?P<password>[0-9a-zA-Z]+))?@(?P<host>[a-zA-Z0-9\-\_\.@]+)(:(?P<port>[1-9][0-9]*))?/(?P<dbname>[0-9a-zA-Z\-_]+)?\s*$')

database_env_re = re.compile("(db|database|host|server)",re.IGNORECASE)
database_user_re = re.compile("[_\-]?(user[_\-]?name|user[_\-]?account|user|account)[_\-]?",re.IGNORECASE)
database_password_re = re.compile("[_\-]?(password|passwd|pass)[_\-]?",re.IGNORECASE)
database_dbname_re = re.compile("[_\-]?(name|dbname)[_\-]?",re.IGNORECASE)
database_schema_re = re.compile("[_\-]?(schema)[_\-]?",re.IGNORECASE)
database_port_re = re.compile("[_\-]?port[_\-]?",re.IGNORECASE)
database_server_re = re.compile("[_\-]?(host|server)[_\-]?",re.IGNORECASE)

oracle_connection_re = re.compile("^(?P<host>[a-zA-Z0-9_\-\.\@]+)(:(?P<port>[0-9]+))?/(?P<dbname>[a-zA-Z0-9_\-]+)$")


def parse_host(host):
    """
    Return hostname and ip if have
    """
    ip = None
    if ip_re.search(host):
        #ip address
        hostname = host
        ip = host
        domain = None
    elif "." in host:
        hostname,domain = host.split(".",1)
    else:
        hostname = host
        domain = None

    return (hostname,ip)

def analysis_workloadenv(cluster=None,refresh_time=None):
    #parse pgsql connection string
    qs =  models.WorkloadEnv.objects.all()
    if cluster:
        qs = qs.filter(workload__cluster = cluster)

    if refresh_time:
        qs = qs.filter(workload__refreshed__gte = refresh_time)

    logger.debug("Begin to find postgres database connection string from single environment variable")
    qs = qs.order_by("workload","name")
    processed_envs = {}
    for env_obj in qs:
        if not env_obj.value:
            continue
        m = postgres_connection_string_re.search(env_obj.value)
        if not m:
            #not a postgres connection string
            continue
        user = m.group("user")
        password = m.group("password")
        host = m.group("host")
        port = int(m.group("port") or 5432)
        dbname = m.group("dbname")
        #get or create the database server
        #check whether the database is a internal database
        server = models.DatabaseServer.objects.filter(internal_name="{}/{}".format(env_obj.workload.namespace.name,host)).first()
        if not server:
            #not a internal database
            hostname,ip = parse_host(host)
            server = update_databaseserver(hostname,models.DatabaseServer.POSTGRES,port,env_obj.modified,host=host,ip=ip)

        #get or create the database
        database = update_database(server,dbname,env_obj.modified)

        #get or create the database user
        database_user = update_databaseuser(server,user,password,env_obj.modified)

        #creata workloaddatabase
        workload_database = update_workloaddatabase(env_obj.workload,database,database_user,password,env_obj.name,env_obj.modified)
        processed_envs[env_obj.id] = workload_database.id

    logger.debug("Begin to find database connection string from multiple environment variables")
    previous_workload = None
    databases = [[],[],[],[],[],[],[]] #[hosts, ports, dbnames,schemas,users,passwords, envs] ;  each member is a list of tuple (env,prefix,sufix)
    dbconfig = [None,None,None,None,None,None] #host, port, dbname,schema,user,password, each member is a tuple(env name, value)
    last_modified = None
    existing_workload_databases = []

    for env_obj in itertools.chain(qs,[models.WorkloadEnv(workload=models.Workload(id=-1),name='test',value=None)]):
        if not previous_workload:
            previous_workload = env_obj.workload
        elif previous_workload != env_obj.workload:
            if databases[4]:
                #have database users
                config_databases = []
                for user_config in databases[4]:
                    if user_config[3]:
                        #this database user is already processed
                        continue
                    kind = None
                    
                    dbconfig[4] = (user_config[0].name,user_config[0].value)
                    last_modified = user_config[0].modified

                    config_values[0] = config_values[1] = config_values[2] = config_values[3] = config_values[5] = None
                    config_items[0]  = config_items[1]  = config_items[2]  = config_items[3]  = config_items[5]  = None

                    for pos in (0,1,2,3,5):
                        configs = databases[pos]
                        if len(configs[0]) == 1:
                            #has only one configuration, use it directly
                            dbconfig[pos] = (configs[0][0].name,configs[0][0].value)
                            if last_modified < configs[0][0].modified:
                                last_modified = configs[0][0].modified
                        elif len(configs[0]) > 1:
                            for config in configs:
                                #check whether name pattern is matched or not
                                if (
                                    (user_config[1] and config[1] and (user_config[1].startswith(config[1]) or config[1].startswith(user_config[1])))
                                    or (user_config[2] and config[2] and (user_config[2].startswith(config[2]) or config[2].startswith(user_config[2])))
                                ):
                                    #server env variable has the same prefix or sufix as the user variable name
                                    dbconfig[pos] = (config[0].name,config[0].value)
                                    if last_modified < config[0].modified:
                                        last_modified = config[0].modified
                                    break


                    if not dbconfig[0]:
                        #Can't decide the database
                        #try to find a host configuration from more database configrations
                        for server_config in databases[6]:
                            m = oracle_connection_re.search(server_config[0].value)
                            if m:
                                #is a oracle connection
                                matched = True
                                kind = models.DatabaseServer.ORACLE
                                dbconfig[0] = (server_config[0].name,m.group("host"))
                                dbconfig[1] = (None,int(m.group("port") or 1521))
                                dbconfig[2] = (None,m.group("dbname"))
                                if last_modified < server_config[0].modified:
                                    last_modified = server_config[0].modified
                                break

                           hostname,ip = parse_host(server_config[0].value)
                           if models.DatabaseServer.objects.filter(host=hostname).exists():
                               #is a database host
                               dbconfig[0] = (server_config[0].name,server_config[0].value)
                               if last_modified < server_config[0].modified:
                                   last_modified = server_config[0].modified
                                   break

                    if not dbconfig[0]:
                        logger.warning("Can't find the database host ,workload({})={} ,related envs = {}".format( previous_workload.id,previous_workload.name,[i[0] for i in dbconfig if i]))
                    else:
                        hostname,ip = parse_host(dbconfig[0][1])
                        server = models.DatabaseServer.objects.filter(internal_name="{}/{}".format(previous_workload.namespace.name,hostname)).first()
                        if not server:
                            #not a internal database
                            if dbconfig[1]:
                                server = models.DatabaseServer.objects.filter(host=hostname,port=config_values[1][1]).first()
                            elif models.DatabaseServer.objects.filter(host=hostname).count() == 1:
                                server = models.DatabaseServer.objects.filter(host=hostname).first()
                            if server:
                                kind = server.kind
                                if not config_values[1]:
                                    dbconfig[1] = (None,server.port)
                            elif not kind:
                                if any(name in hostname for name in ("pgsql","postgres","postgis","pg")):
                                    kind = models.DatabaseServer.POSTGRES
                                    if not dbconfig[1]:
                                        dbconfig[1] = (None,5432)
                                elif any(name in hostname for name in ("mysql","my")):
                                    kind = models.DatabaseServer.MYSQL
                                    if not dbconfig[1]:
                                        dbconfig[1] = (None,3306)
                                elif any(name in hostname for name in ("oracle","ora")):
                                    kind = models.DatabaseServer.ORACLE
                                    if not dbconfig[1]:
                                        dbconfig[1] = (None,1521)

                            server = update_databaseserver(hostname,kind,dbconfig[1][1],last_modified,host=dbconfig[0][1],ip=ip)

                        #get or create the database
                        database = update_database(server,dbconfig[2][1],last_modified)

                        #get or create the database user
                        database_user = update_databaseuser(server,dbconfig[4][1],dbconfig[5][1],last_modified)

                        #creata workloaddatabase
                        workload_database = update_workloaddatabase(previous_workload,database,database_user,dbconfig[5][1],",".join(c[0] for c in dbconfig if c),last_modified,schema=dbconfig[3][1])
                        existing_workload_databases.append(workload_database.id)

            #delete non-existing workload databases
            del_objs = models.WorkloadDatabase.objects.filter(workload=previous_workload).exclude(id__in=existing_workload_databases).delete()
            if del_objs[0]:
                logger.debug("Delete the databases for workload({}),deleted objects = {}".format(previous_workload,del_objs))

            #clean the existing workload databases
            existing_workload_databases.clear()

            #clean previous data
            for field_list in databases:
                field_list.clear()
            previous_workload = env_obj.workload

        if not env_obj.value:
            continue
        if env_obj.id not in processed_envs and not database_env_re.search(env_obj.name):
            continue

        if env_obj.id in processed_envs:
            existing_workload_databases.append(processed_envs[env_obj.id])
            continue

        for field_re,field_list  in (
            (database_server_re,databases[0]),
            (database_port_re,databases[1]),
            (database_user_re,databases[4]),
            (database_dbname_re,databases[2]),
            (database_schema_re,databases[3]),
            (database_password_re,databases[5]),
            (database_env_re,databases[6])
        ):
            m = field_re.search(env_obj.name)
            if not m:
                continue
            prefix,suffix = env_obj.name.split(m.group(),1)
            field_list.append([env_obj,prefix.lower(),suffix.lower()])
            break

process_func_map = {
    10:update_namespace,
    15:update_configmap,
    20:update_volume,
    30:update_volume_claim,
    40:update_ingress,
    50:update_deployment,
    60:update_statefulset,
    70:update_cronjob,
    75:update_daemonset,
    80:delete_cronjob,
    85:delete_daemonset,
    90:delete_statefulset,
    100:delete_deployment,
    110:delete_ingress,
    120:delete_volume_claim,
    130:delete_volume,
    135:delete_configmap,
    140:delete_namespace
}

def resource_type(status,resource_id):
    if status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and NAMESPACE_RE.search(resource_id):
        return 10
    elif status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and CONFIGMAP_RE.search(resource_id):
        return 15
    elif status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and VOLUMN_RE.search(resource_id):
        return 20
    elif status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and VOLUMN_CLAIM_RE.search(resource_id):
        return 30
    elif status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and INGRESS_RE.search(resource_id):
        return 40
    elif status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and DEPLOYMENT_RE.search(resource_id):
        return 50
    elif status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and STATEFULSET_RE.search(resource_id):
        return 60
    elif status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and CRONJOB_RE.search(resource_id):
        return 70
    elif status in (ResourceConsumeClient.NEW,ResourceConsumeClient.UPDATED,ResourceConsumeClient.NOT_CHANGED) and DAEMONSET_RE.search(resource_id):
        return 75
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and CRONJOB_RE.search(resource_id):
        return 80
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and DAEMONSET_RE.search(resource_id):
        return 85
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and STATEFULSET_RE.search(resource_id):
        return 90
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and DEPLOYMENT_RE.search(resource_id):
        return 100
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and INGRESS_RE.search(resource_id):
        return 110
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and VOLUMN_CLAIM_RE.search(resource_id):
        return 120
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and VOLUMN_RE.search(resource_id):
        return 130
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and CONFIGMAP_RE.search(resource_id):
        return 135
    elif status in (ResourceConsumeClient.LOGICALLY_DELETED,ResourceConsumeClient.PHYSICALLY_DELETED) and NAMESPACE_RE.search(resource_id):
        return 140
    else:
        raise Exception("Not Support. status={}, resource_id={}".format(status,resource_id))

def sort_key(val):
    return resource_type(val[0],val[1][0])


def process_rancher(cluster):
    def _func(status,metadata,config_file):
        process_func = process_func_map[resource_type(status,metadata["resource_id"])]
        if config_file:
            with open(config_file) as f:
                config = yaml.load(f.read())
            with transaction.atomic():
                process_func(cluster,status,metadata,config)

    return _func

def resource_filter(resource_id):
    return True if RANCHER_FILE_RE.search(resource_id) else False

def clean_expired_rancher_data():
    for func in (
        modeldata.clean_expired_deleted_data,
        modeldata.clean_orphan_projects,
        modeldata.clean_orphan_namespaces,
        modeldata.check_aborted_harvester,
        modeldata.clean_expired_harvester,
        modeldata.clean_unused_oss,
        modeldata.clean_unreferenced_vulnerabilities,
        modeldata.clean_unreferenced_images,
        modeldata.set_workload_itsystem
    ):
        try:
            func()
        except:
            logger.error("Failed to call method({}) to clean data.{}".format(func,traceback.format_exc()))

def _harvest(cluster,reconsume=False):
    harvest_result = [None,False]
    def _post_consume(client_consume_status,consume_result):
        now = timezone.localtime()
        if "next_clean_time" not in client_consume_status:
            client_consume_status["next_clean_time"] = timezone.make_aware(datetime(now.year,now.month,now.day)) + timedelta(days=1)
        elif now.hour > 6:
            return
        elif now >= client_consume_status["next_clean_time"]:
            harvest_result[1] = True
            client_consume_status["next_clean_time"] = timezone.make_aware(datetime(now.year,now.month,now.day)) + timedelta(days=1)

    now = timezone.now()
    harvester = models.Harvester(name=harvestername.format(cluster.name),starttime=now,last_heartbeat=now,status=models.Harvester.RUNNING)
    harvester.save()
    message = None
    try:
        if isinstance(cluster,models.Cluster):
            pass
        elif isinstance(cluster,int):
            cluster = models.Cluster.objects.get(id=cluster)
        else:
            cluster = models.Cluster.objects.get(name=cluster)
        if cluster.added_by_log:
            message = "The cluster({}) was created by log. no configuration to harvest".format(cluster.name) 
            harvester.status = models.Harvester.SKIPPED
            harvest_result[0] = ([],[])
            return harvest_result
        
        try:
            with LockSession(get_client(cluster.name),3000,2000) as lock_session:
                now = timezone.now()
                result = get_client(cluster.name).consume(process_rancher(cluster),reconsume=reconsume,resources=resource_filter,sortkey_func=sort_key,stop_if_failed=False,f_post_consume=_post_consume)
                #analysis the workload env.
                logger.debug("Begin to analysis workload env")
                analysis_workloadenv(cluster,now)
                cluster.refreshed = timezone.now()
                if result[1]:
                    if result[0]:
                        message = """Failed to refresh cluster({}),
        {} configuration files were consumed successfully.
        {}
        {} configuration files were failed to consume
        {}"""
                        message = message.format(
                            cluster.name,
                            len(result[0]),
                            "\n        ".join(["Succeed to harvest {} resource '{}'".format(resource_status_name,resource_ids) for resource_status,resource_status_name,resource_ids in result[0]]),
                            len(result[1]),
                            "\n        ".join(["Failed to harvest {} resource '{}'.{}".format(resource_status_name,resource_ids,msg) for resource_status,resource_status_name,resource_ids,msg in result[1]])
                        )
                    else:
                        message = """Failed to refresh cluster({}),{} configuration files were failed to consume
        {}"""
                        message = message.format(
                            cluster.name,
                            len(result[1]),
                            "\n        ".join(["Failed to harvest {} resource '{}'.{}".format(resource_status_name,resource_ids,msg) for resource_status,resource_status_name,resource_ids,msg in result[1]])
                        )
                elif result[0]:
                    message = """Succeed to refresh cluster({}), {} configuration files were consumed successfully.
        {}"""
                    message = message.format(
                        cluster.name,
                        len(result[0]),
                        "\n        ".join(["Succeed to harvest {} resource '{}'".format(resource_status_name,resource_ids) for resource_status,resource_status_name,resource_ids in result[0]])
                    )
                else:
                    message = "Succeed to refresh cluster({}), no configuration files was changed since last consuming".format(cluster.name)
        
                harvester.status = models.Harvester.FAILED if result[1] else models.Harvester.SUCCEED

                if len(result[0]) or len(result[1]):
                    cluster.added_by_log = False
                    cluster.save(update_fields=["added_by_log"])
                harvest_result[0] = result
                return harvest_result
        except exceptions.AlreadyLocked as ex: 
            harvester.status = models.Harvester.SKIPPED
            message = "The previous harvest process is still running.{}".format(str(ex))
            logger.info(message)
            harvest_result[0] = ([],[(None,None,None,message)])
            return harvest_result
    except : 
        harvester.status = models.Harvester.FAILED
        message = "Failed to harvest rancher configuration.{}".format(traceback.format_exc())
        logger.info(message)
        harvest_result[0] = ([],[(None,None,None,message)])
        return harvest_result
        
    finally:
        #logger.debug("Begin to refresh rancher workload in web app locaion")
        harvester.message = message
        harvester.endtime = timezone.now()
        harvester.last_heartbeat = harvester.endtime
        harvester.save(update_fields=["endtime","message","status","last_heartbeat"])

def harvest(cluster,reconsume=False):
    harvest_result = _harvest(cluster,reconsume=reconsume)
    if harvest_result[1]:
        clean_expired_rancher_data()

    return harvest_result[0]

def harvest_all(reconsume=False):
    consume_results = []
    need_clean = False
    for cluster in models.Cluster.objects.filter(added_by_log=False):
        harvest_result = _harvest(cluster,reconsume=reconsume)
        if harvest_result[1]:
            need_clean = True
        consume_results.append((cluster,harvest_result[0]))

    if need_clean:
        clean_expired_rancher_data()

    return consume_results
